# DO280 4.2 - Red Hat OpenShift Administration I

## Table of contents
[Chapter 2. Verifying a Cluster](#chapter-2-verifying-a-cluster)  
[Chapter 3. Configuring Authentication](#chapter-3-configuring-authentication)  
[Chapter 4. Controlling Access to OpenShift Resources](#chapter-4-controlling-access-to-openshift-resources)  
[Chapter 5. Configuring OpenShift Networking Components](#chapter-5-configuring-openshift-networking-components)  
[Chapter 6. Controlling Pod Scheduling](#chapter-6-controlling-pod-scheduling)  
[Chapter 7. Scaling an OpenShift Cluster](#chapter-7-scaling-an-openshift-cluster)  
[Chapter 8. Performing Cluster Updates](#chapter-8-performing-cluster-updates)  
[Chapter 9. Managing a Cluster with the Web Console](#chapter-9-managing-a-cluster-with-the-web-console)

## Chapter 2. Verifying a Cluster
Verify health of nodes:
* `oc get nodes` - displays status of each node
* `oc adm top nodes` - dispalys current CPU and memory usage of each node
* `oc describe node <node_name>` - displays resources used and avaiable

Review cluster version resource:
* `export KUBECONFIG=<installdir>/auth/kubeconfig`
* `oc login -u kubeadmin -p <password>` - password listed in `<installdir>/auth/kubeadmin-password`
* `oc get clusterversion` - displays version of cluster
* `oc describe clusterversion` - more detailed information for cluster

Cluster operators:
* `oc get clusteroperators` - retrive list of all cluster operators
* `oc adm node-logs -u crio <nodename>` - Logs for CRI-O container engine
* `oc adm node-logs -u kubelet <nodename>` - Logs for Kubelet
* `oc adm node-logs <nodename>` - Dispalys all logs for a node
* Other components run inside regular pods

Opening a shell on a node:
* `oc debug node/<nodename>` - opens a shell prompt on node
* `crictl` - from inside debug command prompt, shows low-level info about all containers running on node

Troubleshooting:
* `oc get pod` and `oc status` can check if pods and containers are running
* `oc get events` and `oc describe pod` - to view events
* `oc logs <pod>` - display logs in pod (`-c <containter>` if pod has multiple containers)
* `oc debug deployment/<deployment_name> --as-root` - creates pod and optionally overrides default values
* `oc rsh <pod_name>` - opens shell inside pod, can be interative or non-interactive
* `oc cp <filename> <pod_name>:/<filename>` - copies file to pod (reverse also works)
* `oc port-forward <pod_name> <host_port>:<pod_port>` - sets up port forwarding
* `oc ... --log-level <level>` where level is 6 through 10, displays info for request
* `oc whoami -t` - gets authentication token

OCP has cluster logging subsystem based on Elasticsearch, Fluentd or Rsyslog, and Kibana.

### Labs
Some commands in ch02s06 may work, but the latter part of the lab uses a custom project.  You can 
create a broken project similar to the one in the lab:
1. Create a temporary project and deploy the `postgresql` image to it:
   ```
   oc new-project test
   oc new-app --docker-image=registry.access.redhat.com/rhscl/postgresql-96-rhel7:1 \
   --name=psql -e POSTGRESQL_DATABASE=db -e POSTGRESQL_PASSWORD=pass -e POSTGRESQL_USER=user
   ```
1. Export the DeploymentConfig (note: the lab uses a Kubernetes Deployment so some of the commands will be different). You may need to wait
for the application to be fully deployed:
   ```
   oc get dc/psql -o yaml > psql.yaml
   ```
1. Edit the `psql.yaml` file and make the following changes:
   1. Update the namespace by doing a search/replace.  Replace `test` with `execute-troubleshoot`
   1. Find a line which reads: `execute-troubleshoot: false` and delete that and all the remaining lines in the file
   1. In the `spec` section you should see a line:
      ```
      image: registry.access.redhat.com/rhscl/postgresql-96-rhel7@sha256:e1e...
      ```
   1. Remove the `l` from `postgresql` and delete everything after the `@`:
      ```
      image: registry.access.redhat.com/rhscl/postgresq-96-rhel7
      ```
   1. Save the file
1. Create the project for the lab:
   ```
   oc new-project execute-troubleshoot
   ```
1. Import the new (broken) deployment configuration:
   ```
   oc create -f psql.yaml -n execute-troubleshoot
   ```
1. When running the lab use `dc/psql` instead of `deployment/psql`

## Chapter 3. Configuring Authentication
Two ways to authenticate on a just-installed OCP cluster:
* `kubeadmin` virtual user and password
* `kubeconfig` file created in the `auth` directory in the installation folder

To use `kubeconfig`:
```
export KUBECONFIG=<install_dir>/auth/kubeconfig
```

To use `kubeadmin`, see the password in `<install_dir>/auth/kubeadmin-password`.  `kubeadmin` may be
deleted after an identity provider has been configured:  `oc delete secrete kubeadmin -n kube-system`.

* `oc get -o yaml oath cluster` - gets current OAth cluster resource
* `oc replace -f <filename>.yaml` - replaces cluster resource
* `htpasswd -c -B -b <filename> <userid> <password>` - create/update password file
* `htpasswd -b <filename> <userid> <password>` - add/update credentials
* `htpasswd -D <filename> <userid>` - delete credentials
* `oc create secret generic htpasswd-secret --from-file htpasswd=<filename> -n openshift-config` - update configuration to use password file
* Add `--dry-run` to above command and pipe to `oc replace -n openshift-config -f -` to update configuration if users are added, changed, or deleted
* `oc extract secret/htpasswd-secret -n openshift-config --to -` - extracts data from secret

When deleting users:
* `oc delete user <userid>` - removes user resource
* `oc get identities | grep <userid>` - display resources
* `oc delete identity my_htpasswd_provider:<userid>` - delete resources
* `oc delete identity <userid>`

To assign admin privileges:
* `oc adm policy add-cluster-role-to-user cluster-admin <userid>`

### Labs
Lab should run as documented.  

## Chapter 4. Controlling Access to OpenShift Resources
Role-based Access Control (RBAC):
* `oc adm policy add-cluster-role-to-user <cluster-role> <username>` - add cluster role to user
* `oc adm policy remove-cluster-role-from-user <cluster-role> <username>` - removes role from user
* `oc adm policy who-can delete user` - determines if user can perform specified action
* `oc adm policy add-role-to-user <role-name> <username> -n <project>` - add role to user

| Default role       | Description                           |
| :---               | :---                                  |
| 'admin'            | Manage all project resources          |
| 'basic-user'       | Read access to project                |
| 'cluster-admin'    | Superuser access to cluster resources |
| 'cluster-status'   | Can retrieve cluster status info      |
| 'edit'             | Edit access to project resources      |
| 'self-provisioner' | Can create new projects               |
| 'view'             | View project resources                |

Create a secret:
* `oc create secret generic <name> --from-literal key1=secret1` - create secret object.  Can include additional `--from-literal` entries
* `oc secrets add --for mount serviceaccount/<svc_acct_name> secret/<name>` - update pod service account to mount secret
* Next, create a pod that consumes the secret as environment varible or data volume

As environment variables:
```
env:
  - name: MYSQL_ROOT_PASSWORD
    valueFrom:
      secretKeyRef:
        name: demo-secret
        key: root_password
```
also:
```
oc set env dc/demo --from=secret/demo-secret
```

As files in pod:
```
oc set volum dc/demo --add --type=secret --secret-name=demo-secret --mount-path=/app-secrets
```

Security Context Constraints (SCC):
* `oc get scc` - lists SCCs defined by OCP
* `oc describe scc <scc>` - shows detailed information about SCC
* `oc create serviceaccount <svc_acct_name>` - creates a service account
* `oc adm policy add-scc-to-user SCC -z <svc_acct>` - associates service account with SCC
* `oc get pod <pod> -o yaml | oc adm policy scc-subject-review -f -` - identify which account can create a pod with elevated security

### Labs
I didn't replicate the ch04s02 lab in the local environment.  The ch04s04 lab should run as-is.

## Chapter 5. Configuring OpenShift Networking Components
DNS:
* `oc describe dns.operator/default` - sows config for DNS operator

Cluster network operator:
* `oc get Network.config.openshift.io cluster -o yaml` - list SDN configuration

Creating routes:
* `oc expose service <service> --hostname <hostname>` - expose insecure route.  Hostname generated if not specified
* `oc create route edge --service <service> --api.apps.acme.com --key api.key --cert api.crt` - create secure edge route

## Chapter 6. Controlling Pod Scheduling
Labeling nodes:
* `oc label node <node> <label>=<value>` - adds a node label.  Use `--overwrite` to change an existing label.  `<label>-` removes a label
* `oc get node <node> --show-labels` - displays node's labels
* `oc get node -L <label>` - display value of label on each node.  May use `-L` multiple times
* `oc get machines -n openshift-machine-api -o wide` - list machines and nodes
* `oc get machineset -n openshift-machine-api` - list machineset names
* `oc edit machineset <machine_set> -n openshift-machine-api` - edit the machineset configuration.  Labels may be added here

Pod placement:
* `oc edit deployment/<app>` - edit deployment config.  Specify `nodeSelector`
* `oc adm new-project <project> --node-selector "<label>=<value>"` - create new project with node selector set
* `oc annotate namespace <project> openshift.io/node-selector="<label>=<value>" --overwrite` - modifies existing project
* `oc scale --replicas 3 deployment/<app>` - specifies number of pods for app
* `oc get deployment` - show current deployment 

Resource limits:
* `oc set resources deployment <app> --requests cpu-10m,memory=20Mi --limits cpu=80m,memory=100Mi` - set limits on deployment
* `oc describe node <node>` - displays currentl limits and other info
* `oc adm top nodes -l node-role.kubernetes.io/worker` - show resources used on worker nodes
* `oc adm top pods` - show resources used on pods

Resources which may be restricted: `pods`, `replicationcontrollers`, `services`, `secrets`, `persistentvaolumeclaims`.  
Compute resources which may be restricted: `cpu` (`requests.cpu`, in millicores), `memory` (`requests.memory`), `storage` (`requests.storage`). `limits.` prefix for limits instead of requests.

* `oc create --save-config -f dev-quota.yaml` - define resource quotas with yaml file
* `oc create quota <quota_name> --hard services=10,cpu=1300,memory=1.5Gi` - define resource quota from command line
* `oc get resourcequota` - list quotas
* `oc describe quota` - more details on quotas
* `oc delete resourcequota <quata_name>` - delete a quota
* `oc describe limitrange <limit_name>` - more details on limits
* `oc delete limitrange <limit_name>` - delete a limit range
* `oc create clusterquota <quota_name> --project-annotation-selector openshift.io/requester=<user> --hard pods=12,secrets=20` - apply to multiple projects by owner
* Use `--project-label-selector <label>=<value>` to assign quota by label
* `oc delete clusterquota <quota_name>` - delete cluster quota
* `oc autoscale dc/<app> --min <x> --max <y> --cpu-percent <z>` - create horizontal pod autoscaler
* `oc get hpa` - show current autoscale information

### Labs
In ch06s04 lab the application is created and limits for CPU and memory applied.  Depending on your configuration you may need to adjust some
of the limits accordingly to get it to work.  Use the `oc adm top nodes -l node-role.kubernetes.io/worker` command to see what resources are 
available on your worker nodes.

## Chapter 7. Scaling an OpenShift Cluster

## Chapter 8. Performing Cluster Updates
* `oc get clusterversion` - show current version
* `oc adm upgrade` - view available updates
* `oc adm upgrade --to-latest=true` - install latest updated.  Use `to=<version>` to specify version

## Chapter 9. Managing a Cluster with the Web Console
* `oc get routes -n openshift-console` - list URL to web console
